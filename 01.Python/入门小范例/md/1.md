```markdown
@author holemar
@desc 实现 relu
线性整流函数（Linear rectification function），又称修正线性单元，是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。
@date 2024/05/13
```

#### 1 实现 relu

在神经网络中，`relu`作为神经元的激活函数：

```python
def relu(x):
    """
    x: 输入参数
    return：输出relu值
    """
    return max(0,x)                                                                 

# 测试：
relu(5) # 5
relu(-1) # 0
```



[下一个例子](2.md)